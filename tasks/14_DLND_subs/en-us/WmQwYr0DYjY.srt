1
00:00:00,980 --> 00:00:02,870
Hi, there, welcome back.

2
00:00:02,870 --> 00:00:05,929
So this week I'm
going to be talking

3
00:00:05,929 --> 00:00:07,649
about transfer learning.

4
00:00:07,650 --> 00:00:12,679
So in practice most people don't
train their own huge networks

5
00:00:12,679 --> 00:00:15,019
on huge datasets.

6
00:00:15,019 --> 00:00:17,449
So for instance,
if you're training

7
00:00:17,449 --> 00:00:20,250
a convolutional network,
something like AlexNet

8
00:00:20,250 --> 00:00:24,740
on the ImageNet dataset,
it can take multiple weeks

9
00:00:24,739 --> 00:00:26,119
on multiple GPUs.

10
00:00:26,120 --> 00:00:29,450
So it's just not something
that most people have

11
00:00:29,449 --> 00:00:31,250
the time or resources to do.

12
00:00:31,250 --> 00:00:34,520
So instead what you can do
is take a pre-trained network

13
00:00:34,520 --> 00:00:36,670
like AlexNet net
that's been pre-trained

14
00:00:36,670 --> 00:00:40,980
and modify it to use
with your own problem.

15
00:00:40,979 --> 00:00:43,549
So for instance, if
you have some images

16
00:00:43,549 --> 00:00:47,000
that you want to classify
or do some object detection,

17
00:00:47,000 --> 00:00:50,509
you could grab pre-trained
convolutional network

18
00:00:50,509 --> 00:00:54,829
like AlexNet or VGGNet,
and use those instead as

19
00:00:54,829 --> 00:00:57,409
feature extractors
to actually grab

20
00:00:57,409 --> 00:00:59,569
the features from your images.

21
00:00:59,570 --> 00:01:01,250
And then the idea
is that you just

22
00:01:01,250 --> 00:01:05,040
take these last few
fully connected layers.

23
00:01:05,040 --> 00:01:08,359
These are basically just acting
like a classifier, right?

24
00:01:08,359 --> 00:01:10,609
So you just get rid of
those from that network

25
00:01:10,609 --> 00:01:13,590
and you add your own
classifier on top of that.

26
00:01:13,590 --> 00:01:16,219
So there's no book all
be having you implement

27
00:01:16,219 --> 00:01:19,429
some transfer learning to
actually do some image class

28
00:01:19,430 --> 00:01:22,190
classification on a new dataset.

29
00:01:22,189 --> 00:01:26,239
We'll be using the
VGGNet architecture.

30
00:01:26,239 --> 00:01:30,289
I found a GitHub repo
where they've already

31
00:01:30,290 --> 00:01:34,790
pre-trained VGGNet, so we can
just grab that and use it.

32
00:01:34,790 --> 00:01:39,500
So the idea behind
VGGNet is basically this.

33
00:01:39,500 --> 00:01:41,000
I mean you can see
the architecture.

34
00:01:41,000 --> 00:01:43,123
So we have a bunch of
convolutional layers and then

35
00:01:43,123 --> 00:01:46,081
a max-pooling, convolution,
max-pooling, convolution,

36
00:01:46,081 --> 00:01:47,039
max-pooling, and so on.

37
00:01:47,040 --> 00:01:51,230
So we have five of these kind of
convolutional layers going up.

38
00:01:51,230 --> 00:01:54,109
And then at the end we have
three fully connected layers

39
00:01:54,109 --> 00:01:56,885
and then a softmax to
do our classification.

40
00:01:56,885 --> 00:02:00,140
So what we're going to do is
use these convolutional layers

41
00:02:00,140 --> 00:02:01,700
as feature extractors.

42
00:02:01,700 --> 00:02:04,310
Then we're going to toss out
these fully connected layers

43
00:02:04,310 --> 00:02:08,419
and add our own, so that
we can train classifier

44
00:02:08,419 --> 00:02:10,439
for our new images.

45
00:02:10,439 --> 00:02:12,979
So here, this fully
connected layer

46
00:02:12,979 --> 00:02:15,409
after all these
convolutional layers,

47
00:02:15,409 --> 00:02:19,310
it'll be a layer
with 4,096 units

48
00:02:19,310 --> 00:02:23,060
and then we can just use the
activations of all these units

49
00:02:23,060 --> 00:02:24,539
as a code, right?

50
00:02:24,539 --> 00:02:26,659
So it's just kind of
like a convolutional code

51
00:02:26,659 --> 00:02:30,259
that we can use as
input to our classifier.

52
00:02:30,259 --> 00:02:33,049
So our goal here is to
take each of our images

53
00:02:33,050 --> 00:02:34,546
in our new dataset,
pass it through

54
00:02:34,545 --> 00:02:36,169
these convolutional
layers, and get out

55
00:02:36,169 --> 00:02:38,849
this code in this layer.

56
00:02:38,849 --> 00:02:41,900
And this is something you'll
be implementing later on.

57
00:02:41,900 --> 00:02:43,969
So you can read more
about transfer learning

58
00:02:43,969 --> 00:02:47,569
and some other ways to do
this in these course notes

59
00:02:47,569 --> 00:02:49,689
from CS231N.

60
00:02:49,689 --> 00:02:50,840
They're pretty good.

61
00:02:50,840 --> 00:02:52,700
So yeah, just be sure
to check those out

62
00:02:52,699 --> 00:02:55,369
if you want to learn
more about this topic.

63
00:02:55,370 --> 00:02:57,710
So for this
implementation, we're

64
00:02:57,710 --> 00:03:02,250
going to be using code
from this repository.

65
00:03:02,250 --> 00:03:04,250
So TensorFlow VVG.

66
00:03:04,250 --> 00:03:09,289
And this is a VVG network
that has already been trained.

67
00:03:09,289 --> 00:03:13,259
And you can download
the parameters.

68
00:03:13,259 --> 00:03:17,269
So what you'll need to do is
actually clone this repository

69
00:03:17,270 --> 00:03:18,439
before you start.

70
00:03:18,439 --> 00:03:20,650
And in this cell
down here is where

71
00:03:20,650 --> 00:03:23,340
you're going to be
actually downloading

72
00:03:23,340 --> 00:03:25,890
the parameters for this model.

73
00:03:25,889 --> 00:03:28,039
So make sure you do this.

74
00:03:28,039 --> 00:03:31,639
And so download, so
clone the repository

75
00:03:31,639 --> 00:03:34,129
into the same folder where
this notebook is and then

76
00:03:34,129 --> 00:03:35,750
use this to download
the parameters

77
00:03:35,750 --> 00:03:37,340
into that same folder.

78
00:03:37,340 --> 00:03:40,129
And this should all set
it up and do it for you.

79
00:03:40,129 --> 00:03:42,799
And for this one
we're going to be

80
00:03:42,800 --> 00:03:45,260
classifying images of flowers.

81
00:03:45,259 --> 00:03:50,569
So this dataset is actually
from the TensorFlow Inception

82
00:03:50,569 --> 00:03:52,451
Transfer Learning tutorial.

83
00:03:52,451 --> 00:03:54,409
So I'm using a different
network than they are,

84
00:03:54,409 --> 00:03:56,060
but I grabbed the same dataset.

85
00:03:56,060 --> 00:03:59,060
I find that this is typically
a good way to go about it,

86
00:03:59,060 --> 00:04:02,189
because if you know a dataset
works for your problem,

87
00:04:02,189 --> 00:04:04,349
but you have something
different about it,

88
00:04:04,349 --> 00:04:07,349
it's usually good to have, only
change one thing at a time.

89
00:04:07,349 --> 00:04:10,473
So if I know the dataset works,
then and if there's a problem,

90
00:04:10,473 --> 00:04:12,139
then I know it's
something in my network

91
00:04:12,139 --> 00:04:13,909
and not the dataset itself.

92
00:04:13,909 --> 00:04:16,250
Which is why I chose to
use this flower dataset.

93
00:04:16,250 --> 00:04:18,439
So you can read
more about it there.

94
00:04:18,439 --> 00:04:20,689
You can see, you can
read their tutorial

95
00:04:20,689 --> 00:04:24,709
for doing transfer learning
with their interception network.

96
00:04:24,709 --> 00:04:27,859
And here is where we
download the dataset.

97
00:04:27,860 --> 00:04:30,490
So this will just do it for you.

